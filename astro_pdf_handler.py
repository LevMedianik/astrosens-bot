import os
from pdfminer.high_level import extract_text
import docx
from uuid import uuid4
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from dotenv import load_dotenv
import requests

load_dotenv()

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
DATA_DIR = "./data"
INDEX_DIR = "./faiss_index"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(INDEX_DIR, exist_ok=True)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º—ã–π —Ñ–∞–π–ª
def save_file(file_bytes, original_filename):
    file_id = str(uuid4())
    filename = f"{file_id}_{original_filename}"
    filepath = os.path.join(DATA_DIR, filename)
    with open(filepath, 'wb') as f:
        f.write(file_bytes)
    return filepath

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
def extract_text_from_pdf(filepath):
    return extract_text(filepath)

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX
def extract_text_from_docx(filepath):
    doc = docx.Document(filepath)
    return '\n'.join([para.text for para in doc.paragraphs])

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ TXT
def extract_text_from_txt(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return f.read()

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
def extract_text_from_file(filepath):
    if filepath.endswith(".pdf"):
        return extract_text_from_pdf(filepath)
    elif filepath.endswith(".docx"):
        return extract_text_from_docx(filepath)
    elif filepath.endswith(".txt"):
        return extract_text_from_txt(filepath)
    else:
        raise ValueError("‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF, DOCX –∏ TXT —Ñ–∞–π–ª—ã.")

# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
def index_text_with_faiss(text):
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    chunks = splitter.split_text(text)
    documents = [Document(page_content=chunk) for chunk in chunks]

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(documents, embedding=embeddings)
    vectorstore.save_local(INDEX_DIR)
    return vectorstore

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
def load_existing_index():
    index_file = os.path.join(INDEX_DIR, "index.faiss")
    if not os.path.exists(index_file):
        return None
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "HTTP-Referer": "http://localhost",
    "X-Title": "AstroSens"
}
LLM_MODEL = "deepseek/deepseek-chat-v3-0324:free"

# –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
def ask_llm(prompt: str):
    body = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": "–¢—ã ‚Äî –Ω–∞—É—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∞—Å—Ç—Ä–æ–Ω–æ–º–∏–∏ –∏ –∞—Å—Ç—Ä–æ—Ñ–∏–∑–∏–∫–µ."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }
    response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=HEADERS, json=body)
    if response.status_code != 200:
        return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: {response.status_code} - {response.text}"
    return response.json()['choices'][0]['message']['content']

# –í–æ–ø—Ä–æ—Å –∫ –∏–Ω–¥–µ–∫—Å—É
def query_index(question: str, announce: bool = False):
    vectorstore = load_existing_index()
    if not vectorstore:
        return "‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç."

    retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 4})
    results = retriever.get_relevant_documents(question)
    context = "\n".join([doc.page_content.strip() for doc in results])
    full_prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n---\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
    reply = ask_llm(full_prompt)
    return ("üîç –ò—â—É –æ—Ç–≤–µ—Ç...", reply) if announce else reply

# –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
def summarize_pdf(announce: bool = False):
    vectorstore = load_existing_index()
    if not vectorstore:
        return "–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ PDF-–¥–æ–∫—É–º–µ–Ω—Ç."

    retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})
    results = retriever.get_relevant_documents("–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ñ–∞–π–ª–∞")
    text = "\n".join([doc.page_content.strip() for doc in results])
    prompt = f"–¢—ã ‚Äî –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—é—â–∞—è —Å—Ç–∞—Ç—å—é. –ü–µ—Ä–µ—Å–∫–∞–∂–∏ —ç—Ç–æ –ø–æ–Ω—è—Ç–Ω–æ –∏ —Ç–æ—á–Ω–æ:\n{text}"
    reply = ask_llm(prompt)
    return ("üìñ –ü–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—é —Ç–µ–∫—Å—Ç...", reply) if announce else reply
